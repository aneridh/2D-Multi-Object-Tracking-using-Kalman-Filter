{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4OQ7T3uGvS7",
        "outputId": "9d821103-7c8f-449d-efa6-f674e23134eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
            "100%|██████████| 170M/170M [00:01<00:00, 91.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class KalmanFilter2D:\n",
        "  def __init__(self, init_state, init_uncertainty, process_noise, measurement_noise, dt):\n",
        "\n",
        "    # x: initial position (x, y) and velocities (x_dot, y_dot)\n",
        "    # [x, y, x_dot, y_dot, w, h, w_dot, h_dot]\n",
        "    self.x = np.array(init_state, dtype=float).reshape(8, 1)\n",
        "\n",
        "    # P: state covariance matrix - uncertainty in state estimate\n",
        "    self.P = np.array(init_uncertainty)\n",
        "\n",
        "    # Q: process (covariance/) noise matrix - how much system model can be trusted\n",
        "    self.Q = np.array(process_noise)\n",
        "\n",
        "    # R: measurement noise matrix - how much measurement can be trusted\n",
        "    self.R = np.array(measurement_noise)\n",
        "\n",
        "    # dt: time step between detections\n",
        "    self.dt = dt\n",
        "\n",
        "    # A: State transition matrix - describes system dynamics\n",
        "    self.A = np.array([\n",
        "        [1, 0, dt, 0, 0, 0 ,0 ,0],\n",
        "        [0, 1, 0, dt, 0, 0 ,0 ,0],\n",
        "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
        "        [0, 0, 0, 1, 0, 0 ,0 ,0],\n",
        "        [0, 0, 0, 0, 1, 0, dt, 0],\n",
        "        [0, 0 ,0 ,0 ,0, 1 ,0, dt],\n",
        "        [0, 0, 0, 0, 0, 0, 1, 0],\n",
        "        [0, 0, 0, 0, 0, 0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    # Measurement Matrix\n",
        "    self.H = np.array([\n",
        "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
        "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 1., 0., 0.]])\n",
        "\n",
        "   # Identity matrix\n",
        "    self.I = np.eye(8)\n",
        "\n",
        "\n",
        "  def predict(self):\n",
        "    # predicting based on system dynamics (A) -> x_pred = A * x\n",
        "    self.x = np.dot(self.A, self.x)\n",
        "\n",
        "    # predicting state covariance -> how much can we trust state estimate. NOTE: A instead of measurement function (H) because state variables (x, y) are directly measured\n",
        "    self.P = np.dot(np.dot(self.A, self.P), self.A.T) + self.Q\n",
        "\n",
        "  def update(self, measurement):\n",
        "    # Innovation or measurement residual\n",
        "    y = measurement.reshape(-1, 1) - self.H.dot(self.x)  # Reshape measurement to column vector\n",
        "    # S:  combining prediction uncertainty with measurement noise\n",
        "    S = self.H.dot(self.P).dot(self.H.T) + self.R\n",
        "    # K: kalman gain -> compute\n",
        "    K = self.P.dot(self.H.T).dot(np.linalg.inv(S))\n",
        "    # update state estimate with measurement\n",
        "    self.x = self.x + K.dot(y)\n",
        "    # update state covariance (P)\n",
        "    self.P = (self.I - K.dot(self.H)).dot(self.P)\n",
        "\n",
        "  def get_state(self):\n",
        "    return self.x.flatten()\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# Make sure you are running this in a Python environment where PyTorch and torchvision are properly installed.\n",
        "# You can choose a different model like 'maskrcnn_resnet50_fpn' for instance:\n",
        "\n",
        "model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()  # Set the model to inference mode\n",
        "\n",
        "\n",
        "def iou(boxA, boxB):\n",
        "    # Convert (x, y, w, h) to (x_min, y_min, x_max, y_max)\n",
        "    boxA_x_max = boxA[0] + boxA[2]\n",
        "    boxA_y_max = boxA[1] + boxA[3]\n",
        "    boxB_x_max = boxB[0] + boxB[2]\n",
        "    boxB_y_max = boxB[1] + boxB[3]\n",
        "\n",
        "    # Determine the coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA_x_max, boxB_x_max)\n",
        "    yB = min(boxA_y_max, boxB_y_max)\n",
        "\n",
        "    # Compute the area of intersection\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "\n",
        "    # Compute the area of both the prediction and ground-truth rectangles\n",
        "    boxAArea = (boxA[2]) * (boxA[3])  # Use w and h directly for area calculation\n",
        "    boxBArea = (boxB[2]) * (boxB[3])  # Use w and h directly for area calculation\n",
        "\n",
        "    # Compute the area of union\n",
        "    unionArea = boxAArea + boxBArea - interArea\n",
        "\n",
        "    # Compute the IoU by dividing the intersection area by the union area\n",
        "    iou = interArea / float(unionArea)\n",
        "\n",
        "    # Return the IoU value\n",
        "    return iou\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(r'/content/pexels_videos_2103099 (2160p).mp4')\n",
        "#cap = cv2.VideoCapture(r'/content/traffic_clipped2.mp4')\n",
        "if not cap.isOpened():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    raise IOError('Cant open the video')\n",
        "\n",
        "font_scale = 3\n",
        "font = cv2.FONT_HERSHEY_PLAIN\n",
        "\n",
        "# Load the pre-trained Mask R-CNN model\n",
        "model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Function to transform frames\n",
        "def transform_image(frame):\n",
        "    image = Image.fromarray(frame)\n",
        "    image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
        "    return image\n",
        "\n",
        "color_map = {}\n",
        "unique_id = 0\n",
        "tracked_objects = []\n",
        "\n",
        "MAX_AGE = 10  # Maximum age before a track is removed\n",
        "HIT_THRESHOLD = 3  # Number of hits before a track is confirmed\n",
        "\n",
        "frame_count = 0  # Initialize a frame counter\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    frame_count += 1  # Increment frame counter\n",
        "    dt = 1.0 / cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "\n",
        "    # Predict the next state for each tracked object\n",
        "    for obj in tracked_objects:\n",
        "        obj['kalman_filter'].predict()\n",
        "\n",
        "    # Process every 3 frames, since real life detection won't happen at as high a framerate as the video\n",
        "    if frame_count % 3 == 0:\n",
        "\n",
        "        # Preprocess the frame for the object detection model\n",
        "        transformed_frame = transform_image(frame)\n",
        "\n",
        "        # Perform inference\n",
        "        with torch.no_grad():\n",
        "            prediction = model(transformed_frame)[0]\n",
        "\n",
        "        # Extract bounding boxes and scores from the prediction\n",
        "        boxes = prediction['boxes'].cpu().data.numpy()\n",
        "        boxes = boxes.round().astype(int)\n",
        "        scores = prediction['scores'].cpu().data.numpy()\n",
        "\n",
        "        # Filter out low-confidence detections\n",
        "        confidence_threshold = 0.3\n",
        "        boxes = boxes[scores > confidence_threshold].astype('int')\n",
        "        scores = scores[scores > confidence_threshold]\n",
        "\n",
        "        updated_tracked_objects = []\n",
        "        for box in boxes:\n",
        "\n",
        "            # Find the best matching tracked object\n",
        "            best_match = None\n",
        "            highest_iou = 0.0\n",
        "            #print(type(box))\n",
        "\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            x = x_min  # Top-left x-coordinate\n",
        "            y = y_min  # Top-left y-coordinate\n",
        "            w = x_max - x_min  # Width calculation\n",
        "            h = y_max - y_min  # Height calculation\n",
        "            new_box = np.array((x, y, w, h))\n",
        "\n",
        "\n",
        "            try:\n",
        "                tracked_objects\n",
        "            except NameError:\n",
        "                tracked_objects = []  # Initialize it if it doesn't exist\n",
        "\n",
        "            if tracked_objects != []:\n",
        "              #print(\"tracked_objects defined\")\n",
        "              for tobj in tracked_objects:\n",
        "                  iou_score = iou(tobj['predicted_box'], new_box)\n",
        "                  if iou_score > highest_iou:\n",
        "                      best_match = tobj\n",
        "                      highest_iou = iou_score\n",
        "\n",
        "            if highest_iou > 0.3 and best_match is not None:  # If a match with reasonable IoU is found, update it\n",
        "                best_match['kalman_filter'].update(new_box)  # Only x, y, w, h are used for update\n",
        "\n",
        "                print(best_match['predicted_box'])\n",
        "                print([best_match['kalman_filter'].get_state()[i] for i in [0, 1, 4 ,5]])\n",
        "                best_match['predicted_box'] = np.array([best_match['kalman_filter'].get_state()[i] for i in [0, 1, 4 ,5]])\n",
        "                best_match['age'] = 0  # Reset the age as it's detected in this frame\n",
        "                best_match['hits'] += 1  # Increment the hits\n",
        "                updated_tracked_objects.append(best_match)\n",
        "            else:  # If no match is found, initialize a new Kalman Filter\n",
        "                position_measurement_noise = 100  # lower value than before for x, y\n",
        "                size_measurement_noise = 500  # slightly higher value than position for width and height\n",
        "\n",
        "                # For process noise, we anticipate potentially rapid changes in velocity and acceleration\n",
        "                velocity_process_noise = 500  # higher value for dx, dy to allow for quick adjustment\n",
        "                size_change_process_noise = 100  # reasonable value for dw, dh to adapt to size change quickly\n",
        "\n",
        "                kf = KalmanFilter2D(init_state=[x, y, 0, 0, w, h, 0, 0],\n",
        "                                    #init_uncertainty = np.diag([1, 1, 1, 1, 10, 10, 1, 1]),\n",
        "                                    init_uncertainty = np.diag([100, 100, 100, 100, 10000, 10000, 100, 100]),\n",
        "                                    process_noise = np.diag([1, 1, velocity_process_noise, velocity_process_noise, 0.5, 0.5, size_change_process_noise, size_change_process_noise]) ** 2,\n",
        "                                    measurement_noise = np.diag([position_measurement_noise] * 2 + [size_measurement_noise] * 2),\n",
        "                                    dt=dt)\n",
        "                obj_id = unique_id\n",
        "                unique_id += 1\n",
        "                color_map[obj_id] = (int(np.random.rand() * 255), int(np.random.rand() * 255), int(np.random.rand() * 255))\n",
        "                updated_tracked_objects.append({'id': obj_id,\n",
        "                                                'kalman_filter': kf,\n",
        "                                                'predicted_box': new_box,\n",
        "                                                'age': 0,\n",
        "                                                'hits': 1})\n",
        "\n",
        "\n",
        "        # Update age for tracks that were not matched in the current frame\n",
        "        for tobj in tracked_objects:\n",
        "            if tobj not in updated_tracked_objects:\n",
        "                tobj['age'] += 1\n",
        "                if tobj['age'] <= MAX_AGE:\n",
        "                    updated_tracked_objects.append(tobj)  # Keep track if it's not too old\n",
        "\n",
        "\n",
        "        # Draw bounding boxes on the frame\n",
        "        for obj_index, obj in enumerate(updated_tracked_objects):\n",
        "            obj_id = obj['id']\n",
        "            #print(obj_id)\n",
        "            #print(color_map)\n",
        "            color = color_map[obj_id]\n",
        "\n",
        "            pred_box = obj['predicted_box'].astype(int)\n",
        "            x, y, w, h = pred_box[:4]  # Extract x, y, width, and height\n",
        "            print(f'{obj_id} ----- COORDS: {x, y, w, h}')\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "\n",
        "            label = f'ID: {obj_id}'\n",
        "            label_size = cv2.getTextSize(label, font, font_scale, 2)[0]\n",
        "            text_x = x\n",
        "            text_y = y - label_size[1] if y - label_size[1] > 10 else y + label_size[1]  # Ensure text is within frame\n",
        "\n",
        "            # Put text on the frame\n",
        "            cv2.putText(frame, label, (text_x, text_y), font, font_scale, color, 2)\n",
        "\n",
        "        # Replace old list of tracked objects with the updated one\n",
        "        tracked_objects = updated_tracked_objects\n",
        "\n",
        "        # Display the frame\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "ENxHA8lBGwLQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b2fc090-42c9-4566-9371-b355abba7854"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZ-IwBcC85lm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}